{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dedf37",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# To get environment variables\n",
    "import os\n",
    "\n",
    "# Make the display a bit wider\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "# To split our transcript into pieces\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Our chat model. We'll use the default which is gpt-3.5-turbo\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "# Prompt templates for dynamic values\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate, # I included this one so you know you'll have it but we won't be using it\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "\n",
    "# To create our chat messages\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cae352a",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = '...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f13dda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/Transcripts/acme_co_v2.txt', 'r') as file:\n",
    "    content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b146c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Transcript:\\n\")\n",
    "print(content[:215]) # Why 215? Because it cut off at a clean line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6b6ceb",
   "metadata": {},
   "source": [
    "Split our documents so we don't run into token issues. Experiment with what chunk size words best for your use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042b7d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\"], chunk_size=2000, chunk_overlap=250)\n",
    "texts = text_splitter.create_documents([content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22c76e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"You have {len(texts)} texts\")\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1415339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your api key should be an environment variable, or else put it here\n",
    "# We are using a chat model in case you wanted to use gpt4\n",
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2950a62d",
   "metadata": {},
   "source": [
    "We're going to start with the vanilla load_summarize_chain to see how it goes.\n",
    "If you want to see the default prompts that are used you can explore the LangChain code. Here are the [map reduce prompts](https://github.com/hwchase17/langchain/blob/master/langchain/chains/summarize/map_reduce_prompt.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57da26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verbose=True will output the prompts being sent to the \n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d2572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.run(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2805165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3758a0e0",
   "metadata": {},
   "source": [
    "### Custom Prompts\n",
    "\n",
    "I'm going to write custom prompts that give the AI more instructions on what role I want it to play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6933934",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"\n",
    "\n",
    "You are a helpful assistant that helps {sales_rep_name}, a sales rep at {sales_rep_company}, summarize information from a sales call.\n",
    "Your goal is to write a summary from the perspective of {sales_rep_name} that will highlight key points that will be relevant to making a sale\n",
    "Do not respond with anything outside of the call transcript. If you don't know, say, \"I don't know\"\n",
    "Do not repeat {sales_rep_name}'s name in your output\n",
    "\n",
    "\"\"\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "human_template=\"{text}\" # Simply just pass the text as a human message\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05042a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages(messages=[system_message_prompt, human_message_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3daced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(llm,\n",
    "                             chain_type=\"map_reduce\",\n",
    "                             map_prompt=chat_prompt\n",
    "                            )\n",
    "\n",
    "# Because we aren't specifying a combine prompt the default one will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ef0a5c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output = chain.run({\n",
    "                    \"input_documents\": texts,\n",
    "                    \"sales_rep_company\": \"Marin Transitions Partner\", \\\n",
    "                    \"sales_rep_name\" : \"Greg\"\n",
    "                   })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca00b447",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fda2fa3",
   "metadata": {},
   "source": [
    " I wanted to change the format of the output without needing the user to do extra prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff62d842",
   "metadata": {},
   "source": [
    "### Promptless changes\n",
    "\n",
    "To do this I'll write a few points about the different output types I would like. However, I'll that I'll expose to the user is a simple selection, radio button, or drop down. (We'll use text for now but you can do this in your app).\n",
    "\n",
    "I want to give the user the option to select between different summary output types.\n",
    "\n",
    "I'll have them pick between:\n",
    "1. One Sentence\n",
    "2. Bullet Points\n",
    "3. Short\n",
    "4. Long\n",
    "\n",
    "I could try to pass these words to the LLM, but I want to be more explicit with it. Plus, giving good instructions is the way to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b52e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_output_options = {\n",
    "    'one_sentence' : \"\"\"\n",
    "     - Only one sentence\n",
    "    \"\"\",\n",
    "    \n",
    "    'bullet_points': \"\"\"\n",
    "     - Bullet point format\n",
    "     - Separate each bullet point with a new line\n",
    "     - Each bullet point should be concise\n",
    "    \"\"\",\n",
    "    \n",
    "    'short' : \"\"\"\n",
    "     - A few short sentences\n",
    "     - Do not go longer than 4-5 sentences\n",
    "    \"\"\",\n",
    "    \n",
    "    'long' : \"\"\"\n",
    "     - A verbose summary\n",
    "     - You may do a few paragraphs to describe the transcript if needed\n",
    "    \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b81dac",
   "metadata": {},
   "source": [
    "Create a new template that takes an additional parameter. I need to put this in the combined prompt so that the LLM will output in my format. If I did this in the map section I would lose the format after the combined prompt was done\n",
    "\n",
    "**Map Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926ec377",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"\n",
    "\n",
    "You are a helpful assistant that helps {sales_rep_name}, a sales rep at {sales_rep_company}, summarize information from a sales call.\n",
    "Your goal is to write a summary from the perspective of Greg that will highlight key points that will be relevant to making a sale\n",
    "Do not respond with anything outside of the call transcript. If you don't know, say, \"I don't know\"\n",
    "\"\"\"\n",
    "system_message_prompt_map = SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "human_template=\"{text}\" # Simply just pass the text as a human message\n",
    "human_message_prompt_map = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt_map = ChatPromptTemplate.from_messages(messages=[system_message_prompt_map, human_message_prompt_map])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7b8c0d",
   "metadata": {},
   "source": [
    "**Combined Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178eb07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"\n",
    "\n",
    "You are a helpful assistant that helps {sales_rep_name}, a sales rep at {sales_rep_company}, summarize information from a sales call.\n",
    "Your goal is to write a summary from the perspective of Greg that will highlight key points that will be relevant to making a sale\n",
    "Do not respond with anything outside of the call transcript. If you don't know, say, \"I don't know\"\n",
    "\n",
    "Respond with the following format\n",
    "{output_format}\n",
    "\n",
    "\"\"\"\n",
    "system_message_prompt_combine = SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "human_template=\"{text}\" # Simply just pass the text as a human message\n",
    "human_message_prompt_combine = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt_combine = ChatPromptTemplate.from_messages(messages=[system_message_prompt_combine, human_message_prompt_combine])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3e9fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(llm,\n",
    "                             chain_type=\"map_reduce\",\n",
    "                             map_prompt=chat_prompt_map,\n",
    "                             combine_prompt=chat_prompt_combine,\n",
    "                             verbose=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e01887",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_selection = 'one_sentence'\n",
    "\n",
    "output = chain.run({\n",
    "                    \"input_documents\": texts,\n",
    "                    \"sales_rep_company\": \"Marin Transitions Partner\", \\\n",
    "                    \"sales_rep_name\" : \"Greg\",\n",
    "                    \"output_format\" : summary_output_options[user_selection]\n",
    "                   })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e65c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
