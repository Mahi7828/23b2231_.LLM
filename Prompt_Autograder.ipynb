{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f812a11",
   "metadata": {},
   "source": [
    "Imported Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58fbee40-a24c-4bfc-9d6b-b4ab779fc4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49d04cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5941c012-b1bf-44b5-8922-ad3aca8c9ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.schema import BaseOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "303b9b90-31ac-44e5-a13e-f3d7526a063d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4df8dac4-b32a-46a1-b0a0-ec71d940e05b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "chat_model = ChatOpenAI(openai_api_key = \"sk-proj-INTJGxHzWPvrrvIzjo74T3BlbkFJ8HXGcVxcHKAT1di61JkN\" , temperature = 0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5f52bc",
   "metadata": {},
   "source": [
    "Here , we give the prompt , where in we make the chat model act as a teacher for checking answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41a30da7-219a-472c-bd9f-2c626e6d1184",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_text = \"\"\"You are a high school history teacher grading homework assignments. \\\n",
    "Based on the homework question indicated by “**Q:**” and the correct answer indicated by “**A:**”, your task is to determine whether the student's answer is correct. \\\n",
    "Grading is binary; therefore, student answers can be correct or wrong. \\\n",
    "Simple misspellings are okay.\n",
    "\n",
    "**Q:** {question}\n",
    "**A:** {correct_answer}\n",
    "\n",
    "**Student's Answer:** {student_answer}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"question\", \"correct_answer\", \"student_answer\"], template = prompt_template_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99151816-35fc-46e5-b748-36d2f32502ad",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RunnableSequence' from 'langchain_core' (C:\\Users\\mahim\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunnableSequence\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'RunnableSequence' from 'langchain_core' (C:\\Users\\mahim\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from langchain_core import RunnableSequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1a5edc",
   "metadata": {},
   "source": [
    "We feed the prompt into the chat model :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "801787c8-b0bd-4cf9-bfd5-8b6baba83982",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sequence = prompt | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aca0ed-a015-42e4-b7c5-3498d2b9d3bf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "question = \"Who was the 35th president of the United States of America?\"\n",
    "correct_answer = \"John F. Kennedy\"\n",
    "student_answer =  \"JFK\"\n",
    "\n",
    "# run chain\n",
    "sequence.invoke({'question':question, 'correct_answer':correct_answer, 'student_answer':student_answer})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c84507",
   "metadata": {},
   "source": [
    "The student answer is feeded , checked among the acceptable answers , based on that it prints right or wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2a1228-9b19-4939-a345-879377ae2d5e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "student_answer_list = [\"John F. Kennedy\", \"JFK\", \"FDR\", \"John F. Kenedy\", \"John Kennedy\", \"Jack Kennedy\", \"Jacqueline Kennedy\", \"Robert F. Kenedy\"]\n",
    "\n",
    "for student_answer in student_answer_list:\n",
    "    print(student_answer + \" - \" + str(sequence.invoke({'question':question, 'correct_answer':correct_answer, 'student_answer':student_answer})))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965f6918",
   "metadata": {},
   "source": [
    "Now we are adding an output parser to gets the output in terms of true/false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a426abc0-bd6e-45e4-8729-fe5fa17730e9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class GradeOutputParser(BaseOutputParser):\n",
    "    \"\"\"Determine whether grade was correct or wrong\"\"\"\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return \"wrong\" not in text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92407f49-450f-4be1-908d-2f0d29e00674",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "chain = LLMChain(\n",
    "    llm=chat_model,\n",
    "    prompt=prompt,\n",
    "    output_parser=GradeOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cd32b9-3c2a-446f-aec2-bf370a14a71c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for student_answer in student_answer_list:\n",
    "    print(student_answer + \" - \" + str(chain.run({'question':question, 'correct_answer':correct_answer, 'student_answer':student_answer})))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(conda_env)",
   "language": "python",
   "name": "conda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
